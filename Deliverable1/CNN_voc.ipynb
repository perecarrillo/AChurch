{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDqDj4kC9sFJ"
      },
      "source": [
        "# Master course in Object Recognition\n",
        "## Practice 1\n",
        "\n",
        "### Title: Deep learning advanced architectures\n",
        "\n",
        "The goal is to practice advanced deep learning architectures for multi-label classification in [Pascal VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html). We specifically check ResNet50, Inception and MobileNet. We will see 1) how pretrained ResNet50 on imagenet performs on multi-label images, 2) how to modify classification head and 3) implementation of F1 metric.\n",
        "\n",
        "### NOTES\n",
        "\n",
        "- Hyperparameters are modifiable,\n",
        "- The dataset is PASCAL VOC 2007,\n",
        "- The code uses the KERAS library,\n",
        "- The code can run in google colab.\n",
        "- How to finetune on a pretrained model not included (i.e. freeze the pretrained network and train the head, then finetune everything),\n",
        "- No validation set has been defined. The test and validation sets are the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvj7hqNscPOr"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow.keras.applications as app\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 12\n",
        "per_sample_normalization = True\n",
        "data_augmentation = True\n",
        "net_name = [['resnet50','ResNet50'], ['inception_v3','InceptionV3'], ['mobilenet_v2','MobileNetV2']][0]\n",
        "train_from_scratch = True\n",
        "last_layer_activation = ['softmax', 'sigmoid', None][1]\n",
        "loss = ['categorical_crossentropy', 'binary_crossentropy', 'mean_squared_error', 'mean_absolute_error'][1]\n",
        "img_size = 224\n",
        "num_classes = 20\n",
        "voc_classes = {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}\n",
        "test_imagenet = True\n",
        "root = './VOCdevkit/VOC2007/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgZzH6VNc67k"
      },
      "outputs": [],
      "source": [
        "# Download and untar voc dataset\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "!tar xf VOCtrainval_06-Nov-2007.tar\n",
        "print('VOCtrainval_06-Nov-2007.tar has been uncompressed successfully.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzHI03OwwO_2"
      },
      "outputs": [],
      "source": [
        "# delete this cell and replace the train.txt and test.txt with the given ones\n",
        "files = glob(root+'JPEGImages/*.jpg')\n",
        "n_samples = len(files)\n",
        "print(n_samples)\n",
        "names = []\n",
        "not_seg_idx = []\n",
        "for f, i in zip(files, range(n_samples)):\n",
        "  f = f.split('/')[-1].split('.')[0]\n",
        "  names.append(f)\n",
        "  not_seg_idx.append(i)\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(not_seg_idx)\n",
        "ridx = not_seg_idx[: int(n_samples*0.2)]\n",
        "train_test_split = np.zeros(n_samples)\n",
        "train_test_split[ridx] = 1\n",
        "\n",
        "with open('train.txt', 'w') as ftrn:\n",
        "  with open('test.txt', 'w') as ftst:\n",
        "    for i in range(n_samples):\n",
        "      if train_test_split[i] == 0:\n",
        "        ftrn.write(names[i]+'\\n')\n",
        "      else:\n",
        "        ftst.write(names[i]+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylqAar4PyZdK"
      },
      "outputs": [],
      "source": [
        "# Read and format data\n",
        "def read_content(xml_file: str):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    list_with_all_boxes = []\n",
        "    list_with_all_objects = []\n",
        "    for boxes in root.iter('object'):\n",
        "\n",
        "        classname = boxes.find(\"name\").text\n",
        "        list_with_all_objects.append(voc_classes[classname])\n",
        "\n",
        "        ymin, xmin, ymax, xmax = None, None, None, None\n",
        "\n",
        "        ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
        "        xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
        "        ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
        "        xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
        "\n",
        "        list_with_single_boxes = [xmin, ymin, xmax, ymax]\n",
        "        list_with_all_boxes.append(list_with_single_boxes)\n",
        "\n",
        "    return list_with_all_objects, list_with_all_boxes\n",
        "\n",
        "def load_batch(data_list, step, batch_size, root, img_size):\n",
        "  X, Y = [], []\n",
        "  for f in data_list[step*batch_size : (step+1)*batch_size]:\n",
        "    img = cv2.imread(root + 'JPEGImages/' + f + '.jpg')\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    X.append(img)\n",
        "\n",
        "    classes = np.zeros(num_classes)\n",
        "    cnames, _ = read_content(root + 'Annotations/' + f +'.xml')\n",
        "    for c in cnames:\n",
        "        classes[c] = 1.0\n",
        "    Y.append(classes)\n",
        "\n",
        "  return (np.array(X), np.array(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTyJabOh6nCe"
      },
      "outputs": [],
      "source": [
        "# F1 metric\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_metric(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ej9VbIk_Sfj"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "\n",
        "# Select the corresponding network class\n",
        "mynet = getattr(getattr(app, net_name[0]), net_name[1])\n",
        "\n",
        "# create the base pre-trained model\n",
        "if train_from_scratch:\n",
        "  base_model = mynet(include_top=False)\n",
        "else:\n",
        "  base_model = mynet(weights='imagenet', include_top=False)\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "# and a logistic layer\n",
        "predictions = Dense(num_classes, activation=last_layer_activation)(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "opt_rms = optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(loss=loss, optimizer=opt_rms, metrics=['AUC', f1_metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owwkQVLW6zXD"
      },
      "outputs": [],
      "source": [
        "#data augmentation\n",
        "test_data_gen_args = dict(rescale = None if per_sample_normalization else 1./255,\n",
        "                     samplewise_center=True if per_sample_normalization else False,\n",
        "                     samplewise_std_normalization=True if per_sample_normalization else False)\n",
        "train_data_gen_args = dict(rescale = None if per_sample_normalization else 1./255,\n",
        "                     samplewise_center=True if per_sample_normalization else False,\n",
        "                     samplewise_std_normalization=True if per_sample_normalization else False,\n",
        "                     rotation_range=20,\n",
        "                     width_shift_range=0.1,\n",
        "                     height_shift_range=0.1,\n",
        "                     zoom_range=0.2) if data_augmentation else test_data_gen_args\n",
        "training_datagen = ImageDataGenerator(**train_data_gen_args)\n",
        "test_datagen = ImageDataGenerator(**test_data_gen_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ADgp0js78rS"
      },
      "outputs": [],
      "source": [
        "# Train and test loops\n",
        "\n",
        "# Read the data lists\n",
        "with open('train.txt', 'r') as f:\n",
        "  train_list = f.readlines()\n",
        "  for i in range(len(train_list)):\n",
        "    train_list[i] = train_list[i].rsplit('\\n',1)[0]\n",
        "with open('test.txt', 'r') as f:\n",
        "  test_list = f.readlines()\n",
        "  for i in range(len(test_list)):\n",
        "    test_list[i] = test_list[i].rsplit('\\n',1)[0]\n",
        "n_train_steps = len(train_list) // batch_size\n",
        "n_test_steps = len(test_list) // batch_size\n",
        "\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "train_f1_history = []\n",
        "test_loss_history = []\n",
        "test_acc_history = []\n",
        "test_f1_history = []\n",
        "for epoch in range(n_epochs):\n",
        "  # shuffle the list\n",
        "  random.shuffle(train_list)\n",
        "\n",
        "  # Training loop\n",
        "  _loss = 0\n",
        "  _acc = 0\n",
        "  _f1 = 0\n",
        "  for step in range(n_train_steps):\n",
        "    # Load the batch\n",
        "    X, Y = load_batch(train_list, step, batch_size, root, img_size)\n",
        "\n",
        "    # Perform data augmentation\n",
        "    X, Y = next(training_datagen.flow(X, Y, batch_size=batch_size, shuffle=False))\n",
        "\n",
        "    # Train on one batch\n",
        "    loss, acc, f1 = model.train_on_batch(X, Y)\n",
        "\n",
        "    _loss += loss\n",
        "    _acc += acc\n",
        "    _f1 += f1\n",
        "    train_loss_history.append(loss)\n",
        "    train_acc_history.append(acc)\n",
        "    train_f1_history.append(f1)\n",
        "\n",
        "  # print the log\n",
        "  step += 1\n",
        "  print(\"epoch {0} training loss: {1:.2f}, acc: {2:.2f}, f1: {3:.2f}\".format(epoch, _loss/step, _acc/step, _f1/step))\n",
        "\n",
        "  # Test loop\n",
        "  _loss = 0\n",
        "  _acc = 0\n",
        "  _f1 = 0\n",
        "  for step in range(n_test_steps):\n",
        "    # Load the batch\n",
        "    X, Y = load_batch(test_list, step, batch_size, root, img_size)\n",
        "\n",
        "    # Perform data augmentation\n",
        "    X, Y = next(test_datagen.flow(X, Y, batch_size=batch_size, shuffle=False))\n",
        "\n",
        "    # Train on one batch\n",
        "    loss, acc, f1 = model.evaluate(X, Y, verbose = 0)\n",
        "\n",
        "    _loss += loss\n",
        "    _acc += acc\n",
        "    _f1 += f1\n",
        "    test_loss_history.append(loss)\n",
        "    test_acc_history.append(acc)\n",
        "    test_f1_history.append(f1)\n",
        "\n",
        "  # print the log\n",
        "  step += 1\n",
        "  print(\"epoch {0} test     loss: {1:.2f}, acc: {2:.2f}, f1: {3:.2f}\\n\".format(epoch, _loss/step, _acc/step, _f1/step))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swokmMpZ9sFL"
      },
      "outputs": [],
      "source": [
        "# plot the loss\n",
        "plt.plot(train_loss_history, label='train loss')\n",
        "plt.plot(test_loss_history, label='test loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(txt+'_loss')\n",
        "\n",
        "# plot the AUC\n",
        "plt.plot(train_acc_history, label='train auc')\n",
        "plt.plot(test_acc_history, label='test auc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(txt+'_auc')\n",
        "\n",
        "# plot the F1\n",
        "plt.plot(train_f1_history, label='train f1')\n",
        "plt.plot(test_f1_history, label='test f1')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(txt+'_f1')\n",
        "\n",
        "#save model to disk\n",
        "model.save_weights('model.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}